# -*- coding: utf-8 -*-
"""NLP lab 6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CCaadUMn_Io1R-n-a88HG05yjg4YzEry
"""

import nltk
import random
import string
import numpy as np
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

nlp = spacy.load("en_core_web_sm")

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

predefined_responses = {
    "hello": "Hello!",
    "hi": "Hi there!",
    "how are you": "I am doing well, thank you for asking!",
    "what is your name": "I am a chatbot created to assist you.",
    "who are you": "I am a basic chatbot here to help you.",
    "what can you do": "I can answer general questions and assist you with basic topics.",
    "bye": "Goodbye! Have a great day!"
}

corpus = """Hello! How can I help you? I am a basic chatbot.
I can answer questions related to general topics.
I am here to assist you.
I am just a chatbot, so I donâ€™t have emotions, but I am here to help.
You can ask me about general information.
If you have any queries, feel free to ask.
If you need help, let me know.
You can say 'bye' to exit.
Hello!
I am doing well, thank you for asking!
How are you?
I am here to answer your questions.
What is your name?
I am a chatbot created to assist you."""

sent_tokens = nltk.sent_tokenize(corpus)
word_tokens = nltk.word_tokenize(corpus)

def preprocess(text):
    doc = nlp(text.lower())
    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]

def response(user_input):
    sent_tokens.append(user_input)
    vectorizer = TfidfVectorizer(tokenizer=preprocess, stop_words='english')
    tfidf = vectorizer.fit_transform(sent_tokens)
    vals = cosine_similarity(tfidf[-1], tfidf[:-1])
    idx = np.argmax(vals)
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]
    sent_tokens.pop()

    if req_tfidf == 0:
        return "I am sorry! I don't understand."
    else:
        return sent_tokens[idx]

def preprocess(text):
    doc = nlp(text.lower())
    return " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])

def response(user_input):
    user_input = user_input.lower()

    # Check for predefined responses
    for key in predefined_responses.keys():
        if key in user_input:
            return predefined_responses[key]

def response(user_input):
    user_input = user_input.lower()

    # Check for predefined responses
    for key in predefined_responses.keys():
        if key in user_input:
            return predefined_responses[key]

    # Ensure sent_tokens remains unchanged outside this function
    temp_sent_tokens = sent_tokens[:]
    temp_sent_tokens.append(user_input)

    vectorizer = TfidfVectorizer(tokenizer=preprocess, stop_words='english')
    tfidf = vectorizer.fit_transform(temp_sent_tokens)

    vals = cosine_similarity(tfidf[-1], tfidf[:-1])
    idx = np.argmax(vals)
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]

    if req_tfidf == 0:
        return "I am sorry! I don't understand."
    else:
        return temp_sent_tokens[idx]

def chatbot():
    print("Chatbot: Hello! Ask me anything or type 'bye' to exit.")
    while True:
        user_input = input("You: ").lower()
        if user_input == 'bye':
            print("Chatbot: Goodbye!")
            break
        else:
            print("Chatbot:", response(user_input))

# Run chatbot
if __name__ == "__main__":
    chatbot()