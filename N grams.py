# -*- coding: utf-8 -*-
"""nlp4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V1X6GGAGWyR8EgL-fV4Bg9ow29ifX3px
"""

import spacy
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
from collections import Counter
import re
import nltk
from nltk.util import ngrams
from nltk.tokenize import word_tokenize

nlp = spacy.load("en_core_web_sm")

text = """Apple Inc. is planning to invest $1 billion in India. Tim Cook met with Prime Minister Narendra Modi to discuss potential collaborations in the technology sector."""

print("\n===== N-gram Language Model (Bigrams) =====\n")

tokens = word_tokenize(text)
tokens = [token.lower() for token in tokens if re.match(r"^\w+$", token)]

bigrams = list(ngrams(tokens, 2))

bigram_freq = Counter(bigrams)

for bigram, freq in bigram_freq.items():
    print(f"{bigram}: {freq}")

print("\n===== Sequence Labeling (NER using BERT) =====\n")

model_name = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer)

ner_results = ner_pipeline(text)

for entity in ner_results:
    print(f"{entity['word']}: {entity['entity']} (Score: {entity['score']:.4f})")

print("\n===== Syntactic and Dependency Parsing =====\n")

doc = nlp(text)

for token in doc:
    print(f"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}, Head: {token.head.text}")

print("\nNamed Entities (spaCy):")
for ent in doc.ents:
    print(f"{ent.text}: {ent.label_}")